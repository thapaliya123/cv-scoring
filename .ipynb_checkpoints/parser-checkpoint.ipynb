{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5666f891",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "parse JD and CV data \n",
    "'''\n",
    "import io\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.pdfinterp import PDFPageInterpreter\n",
    "from pdfminer.pdfinterp import PDFResourceManager\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "import docx2txt\n",
    "\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    '''\n",
    "    Helper function to extract the plain text from .pdf files\n",
    "    :param pdf_path: path to PDF file to be extracted\n",
    "    :return: iterator of string of extracted text\n",
    "    '''\n",
    "    \n",
    "    with open(pdf_path, 'rb') as fh:\n",
    "        for page in PDFPage.get_pages(fh, \n",
    "                                      caching=True,\n",
    "                                      check_extractable=True):\n",
    "            resource_manager = PDFResourceManager()\n",
    "            fake_file_handle = io.StringIO()\n",
    "            converter = TextConverter(resource_manager, fake_file_handle, codec='utf-8', laparams=LAParams())\n",
    "            page_interpreter = PDFPageInterpreter(resource_manager, converter)\n",
    "            page_interpreter.process_page(page)\n",
    " \n",
    "            text = fake_file_handle.getvalue()\n",
    "            yield text\n",
    " \n",
    "            # close open handles\n",
    "            converter.close()\n",
    "            fake_file_handle.close()\n",
    "\n",
    "def extract_text_from_doc(doc_path):\n",
    "    '''\n",
    "    Helper function to extract plain text from .doc or .docx files\n",
    "    :param doc_path: path to .doc or .docx file to be extracted\n",
    "    :return: string of extracted text\n",
    "    '''\n",
    "    temp = docx2txt.process(doc_path)\n",
    "    text = [line.replace('\\t', ' ') for line in temp.split('\\n') if line]\n",
    "    return ' '.join(text)\n",
    "\n",
    "def extract_text(file_path, extension):\n",
    "    '''\n",
    "    Wrapper function to detect the file extension and call text extraction function accordingly\n",
    "    :param file_path: path of file of which text is to be extracted\n",
    "    :param extension: extension of file `file_name`\n",
    "    '''\n",
    "    text = ''\n",
    "    if extension == '.pdf':\n",
    "        for page in extract_text_from_pdf(file_path):\n",
    "            text += ' ' + page\n",
    "    elif extension == '.docx' or extension == '.doc':\n",
    "        text = extract_text_from_doc(file_path)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "262fdcdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Required skills/Competencies:\\n\\n● Must have at least a bachelor's degree in Computer Science or similar.\\n● Min. 2 years of relevant work experience.\\n● Proficient in Basic Machine Learning concepts: algorithms, evaluation procedures, etc,\\n\\nand dealing with Common failure modes. Experienced in at least one area of application\\n(E.g. CV, NLP, etc)\\n\\n● Has a sound knowledge of mathematical concepts like Linear Algebra, Probability and\\n\\nStatistics, Calculus\\n\\n● Proficient in framework & libraries such as Numpy, Pandas, Matplotlib, Scikit-learn and a\\ngood grasp of at least one of Tensorflow or Pytorch. Familiar with Flask, FastAPI or\\nDjango, and some domain-specific tools (e.g: opencv, spacy, etc)\\n\\n● Good Grasp on programming language and concepts such as Python + OOP + SOLID,\\nData Structures and Algorithms, RESTful APIs, and familiar with Architecture Design\\n● Good Grasp of software tools and platforms such as git, conda, pip, jupyter, Docker, and\\n\\nat least one cloud platform like AWS/GCP\\n\\n● Good grasp of a database such as SQL/NoSQL\\n● Has a good grasp of agile processes like Sprint and Kanban\\n● Good Team Management, Communication, and Problem-Solving Skills\\n\\nResponsibilities:\\n\\n● Develop AI applications to adhere to designs that support business requirements for\\n\\ninternal and external clients.\\n\\n● Research and develop machine learning models and work on the whole ML pipeline:\\n\\ndata collection, wrangling, pre-processing, model building, evaluation, and deployment\\n\\n● Perform data analysis to uncover insights that can be immediately actionable or can\\n\\ninform decisions around the ML process.\\n\\n● Take initiative and ownership in writing requirement specifications and design documents\\n\\nfor a variety of development tasks including feature development, database design, and\\nsystem integrations.\\n\\n● Preparation, drafting, and review of software documentation and project reports to meet\\n\\n● Orchestrate deployment, monitoring, and maintenance of ML applications as per\\n\\ninternal and client requirements.\\n\\nrequirement.\\n\\n● Lead one or more projects in different capacities (if required)\\n● Guide other developers and help them (as required) to do their work and look for ways to\\n\\nimprove overall team output.\\n\\n● Take on Leadership roles (e.g: Supervisorial) as required.\\n\\n\\x0c\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = extract_text(\"./data/jd.pdf\", \".pdf\")\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c247f52",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_340/3320951157.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# text = remove_special_symbols(text)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# text = \" \".join(remove_punctuation(text))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# text.strip()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/f/Fusemachines/FUSE_AI_TALENT/cv_scoring_demo/preprocessing.py\u001b[0m in \u001b[0;36mpreprocess_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;31m# remove punctuation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m     \u001b[0mtokens_without_punct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_punctuation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremoved_special_symbols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;31m# remove stopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/f/Fusemachines/FUSE_AI_TALENT/cv_scoring_demo/preprocessing.py\u001b[0m in \u001b[0;36mremove_stopwords\u001b[0;34m(tokens)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;31m# tokenize text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0mtext_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;31m# remove stop words:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/f/Fusemachines/FUSE_AI_TALENT/cv_scoring_demo/preprocessing.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;31m# tokenize text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0mtext_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;31m# remove stop words:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "import preprocessing\n",
    "from preprocessing import lower_casing, remove_special_symbols, remove_punctuation, preprocess_text\n",
    "\n",
    "# text = remove_special_symbols(text)\n",
    "text = preprocess_text(text)\n",
    "# text = \" \".join(remove_punctuation(text))\n",
    "# text.strip()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05351a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__pycache__  parser.ipynb  preprocessing.py\t   streamlit_main.py  utils.py\r\n",
      "data\t     parser.py\t   skill_db_relax_20.json  token_dist.json    venv\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa8d9d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
